{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Couro-newmodel",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dcE6Jyo7uSYcdNZtAmxeIDj4BKG7z34D",
      "authorship_tag": "ABX9TyMvBkpL8rZLbJrgOQUQMtnB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisFernandoRosa/CNN-Training-Structure/blob/main/Couro_newmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl1NpVo9XRiJ"
      },
      "source": [
        "###**importações**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtPwh5REtdaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2aa9897e-d7b8-4168-fb2d-2c3d28b96260"
      },
      "source": [
        "!pip install --quiet neural_structured_learning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▊                             | 10kB 23.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 20kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 30kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 40kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 51kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 61kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 71kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 81kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 92kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 102kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 112kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 7.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UZKHVMgqAn5"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "import random\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications import ResNet50, MobileNetV2\n",
        "from tensorflow.keras.models import Model, save_model\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.layers import Dense, Dropout, AveragePooling2D, Flatten\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, FalsePositives, AUC, CategoricalAccuracy\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import albumentations as alb\n",
        "import neural_structured_learning as nsl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT9nzYdGvm-w"
      },
      "source": [
        "####**preprocessing**\n",
        "*   Definition of the functions for preprocessing methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSq7YExlaaaf"
      },
      "source": [
        "def preprocessing_original_image(img,cla,batch,l_cla):\n",
        "  img = cv2.resize(img,(224,224), interpolation = cv2.INTER_CUBIC)\n",
        "  batch.append(img)\n",
        "  l_cla.append(to_categorical(cla,Config.class_quant)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CaLd8vRZ8xn"
      },
      "source": [
        "def preprocessing_augmentation(img,cla,batch,l_cla):\n",
        "  rot = alb.augmentations.transforms.Rotate(always_apply= True)\n",
        "  gns = alb.augmentations.transforms.GaussNoise(always_apply=True)\n",
        "  bri = alb.augmentations.transforms.RandomBrightness(always_apply=True)\n",
        "\n",
        "  batch.append(img)\n",
        "  l_cla.append(to_categorical(cla,Config.class_quant)) \n",
        "\n",
        "  np.random.seed(42)\n",
        "  aimg = rot.apply(img,np.random.randint(10,70))\n",
        "  batch.append(aimg)\n",
        "  l_cla.append(to_categorical(cla,Config.class_quant)) \n",
        "  \n",
        "  np.random.seed(42)\n",
        "  aimg = bri.apply(img,np.random.uniform(0.6,0.9))\n",
        "  batch.append(aimg)\n",
        "  l_cla.append(to_categorical(cla,Config.class_quant)) \n",
        "\n",
        "  np.random.seed(42)\n",
        "  aimg = gns.apply(img)\n",
        "  batch.append(aimg)\n",
        "  l_cla.append(to_categorical(cla,Config.class_quant)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRL3Vz7ZXeVg"
      },
      "source": [
        "def preprocessing_4p4(img,cla,batch,l_cla):\n",
        "  simg = img[0:int(img.shape[0]/2) , 0:int(img.shape[1]/2),:]   \n",
        "  simg = cv2.resize(simg,Config.input_hw,cv2.INTER_CUBIC)\n",
        "  batch.append(simg)\n",
        "  l_cla.append(to_categorical(cla, Config.class_quant))\n",
        "\n",
        "  simg = img[int(img.shape[0]/2) : img.shape[0] , 0:int(img.shape[1]/2),:]   \n",
        "  simg = cv2.resize(simg,Config.input_hw,cv2.INTER_CUBIC)\n",
        "  batch.append(simg)\n",
        "  l_cla.append(to_categorical(cla, Config.class_quant))\n",
        "\n",
        "  simg = img[0:int(img.shape[0]/2) , int(img.shape[1]/2) : img.shape[1],:]   \n",
        "  simg = cv2.resize(simg,Config.input_hw,cv2.INTER_CUBIC)\n",
        "  batch.append(simg)\n",
        "  l_cla.append(to_categorical(cla, Config.class_quant))\n",
        "\n",
        "  simg = img[int(img.shape[0]/2) : img.shape[0] , int(img.shape[1]/2) : img.shape[1],:]   \n",
        "  simg = cv2.resize(simg,Config.input_hw,cv2.INTER_CUBIC)\n",
        "  batch.append(simg)\n",
        "  l_cla.append(to_categorical(cla, Config.class_quant))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxc9As1ipyZM"
      },
      "source": [
        "####**pos-processing**\n",
        "* Definition of the funtions that will handle with the outputs of the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okE-OJ1hO54h"
      },
      "source": [
        "def processing_predict_original(pob, id, Config):\n",
        "  Config.predict[0].extend(pob)\n",
        "  Config.predict[1].append(to_categorical(Config.class_dict[Config.data_frame.iloc[id]['targets']],Config.class_quant))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVv07EmgBSHT"
      },
      "source": [
        "def processing_predict_4p4(pob, id, Config):\n",
        "  Config.predict[0].extend(pob)\n",
        "  for q in range(4):\n",
        "    Config.predict[1].append(to_categorical(Config.class_dict[Config.data_frame.iloc[id]['targets']],Config.class_quant))\n",
        "  \n",
        "  Config.multi_method = True\n",
        "  Config.predict_method = [[],[],[],[],[]]\n",
        "  Config.method_name = ['add','mult','likelihood','votação','real']\n",
        "\n",
        "  z = np.zeros(Config.class_quant)\n",
        "  l = np.zeros(Config.class_quant)\n",
        "  o = np.ones(Config.class_quant)\n",
        "  for i in pob:\n",
        "    z = np.add(z,i)\n",
        "    l = np.add(z,np.log(i))\n",
        "    o = np.multiply(o,i)    \n",
        "  l = np.exp(l)\n",
        "\n",
        "  Config.predict_method[0].append(z)\n",
        "  Config.predict_method[1].append(o)\n",
        "  Config.predict_method[2].append(l)\n",
        "  \n",
        "  m = 0\n",
        "  pob = np.argmax(pob,axis=1) \n",
        "  pob = list(pob)\n",
        "  for i in pob:\n",
        "    c = pob.count(i)\n",
        "    m = i if c > m else m\n",
        "  Config.predict_method[3].append(to_categorical(m,Config.class_quant))\n",
        "  Config.predict_method[4].append(to_categorical(Config.class_dict[Config.data_frame.iloc[id]['targets']],Config.class_quant))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiKq0rHop16e"
      },
      "source": [
        "####**Save**\n",
        "* Function to save the confusion matrix and all type of output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WnTHmX_SI4X"
      },
      "source": [
        "def matrix(y_pred,y_real):\n",
        "  y_pred = np.argmax(y_pred,axis=1)\n",
        "  print(y_pred)\n",
        "  y_real = np.argmax(y_real,axis=1)\n",
        "  print(y_real)\n",
        "\n",
        "  cf = confusion_matrix(y_real, y_pred,labels = np.arange(0,Config.class_quant))\n",
        "  fig, ax = plt.subplots(figsize = (10,10))\n",
        "\n",
        "  Cm = ConfusionMatrixDisplay(cf, np.arange(0,Config.class_quant ))\n",
        "\n",
        "  Cm.plot(include_values=True, cmap=\"Blues\", ax= ax, xticks_rotation=\"vertical\")\n",
        "  # plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOes6JPf56gw"
      },
      "source": [
        "def save_mc():\n",
        "  os.mkdir(Config.save_path  + '/matriz-confusao')\n",
        "\n",
        "  if (Config.multi_method):\n",
        "    for i in range(len(Config.method_name) - 1):\n",
        "      matrix(Config.predict_method[i],Config.predict_method[len(Config.method_name) - 1])\n",
        "      plt.savefig(Config.save_path + '/matriz-confusao/cm-' + Config.method_name[i] + '.png', format='png')\n",
        "\n",
        "  matrix(Config.predict[0],Config.predict[1])\n",
        "  plt.savefig(Config.save_path + '/matriz-confusao/cm-pred_real.png', format='png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC3tVUGp_nmO"
      },
      "source": [
        "def save_csv():\n",
        "  os.mkdir(Config.save_path  + '/valores')\n",
        "  if (Config.multi_method):\n",
        "    for i in range(len(Config.method_name) ):\n",
        "      f = open(Config.save_path + '/valores/valores-'+ Config.method_name[i] + '.csv', 'a', newline ='\\n')\n",
        "      w = csv.writer(f , delimiter = ',')\n",
        "      for r in range(len(Config.predict_method[i])):\n",
        "        w.writerow(Config.predict_method[i][r])\n",
        "      f.close()\n",
        "\n",
        "  f = open(Config.save_path + '/valores/valores-predP.csv', 'a', newline ='\\n')\n",
        "  w = csv.writer(f , delimiter = ',')\n",
        "  for r in range(len(Config.predict[0])):\n",
        "    w.writerow(Config.predict[0][r])\n",
        "  f.close()\n",
        "\n",
        "  f = open(Config.save_path + '/valores/valores-realP.csv', 'a', newline ='\\n')\n",
        "  w = csv.writer(f , delimiter = ',')\n",
        "  for r in range(len(Config.predict[1])):\n",
        "    w.writerow(Config.predict[1][r])\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuAm4LmocYeH"
      },
      "source": [
        "##**Config**\n",
        "* Configuration, and creating data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t75pop36tRMm"
      },
      "source": [
        "class Config(object):\n",
        "  def __init__(self):\n",
        "    self.model_name = 'Mobile-1'\n",
        "    self.save_path = '/content/drive/My Drive/models/' + self.model_name + '-' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    self.base_path = '/content/drive/My Drive/Base/Florestais/Especies Pericia Federal2'\n",
        "    self.fold_num = 10\n",
        "    self.epochs = 50\n",
        "    self.batch_size = 36\n",
        "    self.optmizer = Adagrad()\n",
        "    self.adversarial = False\n",
        "    self.validation_split = 0.2\n",
        "\n",
        "    self.base_model = 0\n",
        "    self.input_shape = (224,224,3)\n",
        "    self.predict_function = processing_predict_4p4\n",
        "    self.preprocessing_function = preprocessing_3p3\n",
        "    self.loss = CategoricalCrossentropy()\n",
        "    self.metrics = [CategoricalAccuracy(), Precision(), FalsePositives(), AUC()]\n",
        "    self.multi_method = True\n",
        "\n",
        "    self.seed = 5\n",
        "    self.freeze = True\n",
        "    self.img_quant = 0\n",
        "    self.data_frame = None\n",
        "    self.class_quant = 0\n",
        "    \n",
        "    self.class_dict = {}\n",
        "    self.ext = ['png','jpg','jpeg']\n",
        "    self.input_hw = (self.input_shape[1],self.input_shape[0])\n",
        "    self.predict = [[],[]]\n",
        "    self.predict_method = None\n",
        "    self.method_name = None\n",
        "    self.initializer = tf.keras.initializers.glorot_normal(seed = self.seed)\n",
        "\n",
        "\n",
        "    os.mkdir(self.save_path)\n",
        "\n",
        "  def create_data_frame(self, base_path):\n",
        "    imagens = []\n",
        "    classes = []\n",
        "\n",
        "    os.chdir(base_path)\n",
        "    class_dir = os.listdir('.')\n",
        "    class_dir = [fn for fn in class_dir if len(fn.split('.')) < 2]\n",
        "    class_dir = sorted(class_dir)\n",
        "    self.class_quant = len(class_dir)\n",
        "\n",
        "    for class_name, class_num in zip(class_dir, range(self.class_quant)):\n",
        "      cla = [fn for fn in os.listdir(class_name) if fn.split('.')[-1] in self.ext]\n",
        "      for img in cla:\n",
        "        imagens.append(base_path + '/' + class_name + '/' + img)\n",
        "        classes.append(class_name)\n",
        "      self.class_dict[class_name] = class_num\n",
        "\n",
        "    self.data_frame = pd.DataFrame({'inputs' : imagens, 'targets' : classes})\n",
        "    self.img_quant = self.data_frame.shape[0]\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nyBJ92-ui-6"
      },
      "source": [
        "##**Generator**\n",
        "* Generator of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bKslz985tcu"
      },
      "source": [
        "class Generator(object):\n",
        "  def __init__(self, Config, indice_treinamento, indice_teste, validation_split, batch_size):\n",
        "    self.train = None\n",
        "    self.val = None\n",
        "    self.control = 0\n",
        "    self.control1 = 0\n",
        "\n",
        "    np.random.shuffle(indice_treinamento)\n",
        "\n",
        "    if validation_split != None:\n",
        "      self.train, self.val = train_test_split(indice_treinamento, test_size = validation_split)\n",
        "    else:\n",
        "      self.train = indice_treinamento\n",
        "\n",
        "  def train_generator(self,Config):\n",
        "    batch = []\n",
        "    l_cla  = []\n",
        "\n",
        "    while True:\n",
        "      img = cv2.imread(Config.data_frame.iloc[self.train[self.control]]['inputs'])\n",
        "      img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "      c = Config.class_dict[Config.data_frame.iloc[self.train[self.control]]['targets']]\n",
        "\n",
        "      Config.preprocessing_function(img,c,batch,l_cla)\n",
        "\n",
        "      if len(batch) == Config.batch_size:\n",
        "        batch = np.array(batch, np.float32) / 255.\n",
        "        yield (np.array(batch, np.float32),np.array(l_cla, np.float32))\n",
        "        batch = []\n",
        "        l_cla  = []\n",
        "      self.control += 1\n",
        "      if self.control == self.train.shape[0]:\n",
        "        self.control = 0\n",
        "\n",
        "  def val_generator(self,Config):\n",
        "    batch = []\n",
        "    l_cla  = []\n",
        "\n",
        "    while True:\n",
        "      img = cv2.imread(Config.data_frame.iloc[self.val[self.control1]]['inputs'])\n",
        "      img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "      c = Config.class_dict[Config.data_frame.iloc[self.val[self.control1]]['targets']]\n",
        "\n",
        "      Config.preprocessing_function(img,c,batch,l_cla)\n",
        "\n",
        "      if len(batch) == Config.batch_size:\n",
        "        batch = np.array(batch, np.float32) / 255.\n",
        "        yield (np.array(batch, np.float32),np.array(l_cla,np.float32))\n",
        "        batch = []\n",
        "        l_cla  = []\n",
        "      self.control1 += 1\n",
        "      if self.control1 == self.val.shape[0]:\n",
        "        self.control1 = 0\n",
        "  \n",
        "  def adversarial_train_gernerator(self, Config):\n",
        "    batch = []\n",
        "    l_cla  = []\n",
        "\n",
        "    while True:\n",
        "      img = cv2.imread(Config.data_frame.iloc[self.train[self.control]]['inputs'])\n",
        "      img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "      c = Config.class_dict[Config.data_frame.iloc[self.train[self.control]]['targets']]\n",
        "\n",
        "      preprocessing_original_image(img,c,batch,l_cla)\n",
        "\n",
        "      if len(batch) == Config.batch_size:\n",
        "        batch = np.array(batch, np.float32) / 255.\n",
        "        yield {\"inputs\" : np.array(batch, np.float32),\"label\" : np.array(l_cla,np.float32)}\n",
        "        batch = []\n",
        "        l_cla  = []\n",
        "      self.control += 1\n",
        "      if self.control == self.train.shape[0]:\n",
        "        self.control = 0\n",
        "\n",
        "  def adversarial_val_generator(self, Config):\n",
        "    batch = []\n",
        "    l_cla  = []\n",
        "\n",
        "    while True:\n",
        "      img = cv2.imread(Config.data_frame.iloc[self.val[self.control1]]['inputs'])\n",
        "      img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "      c = Config.class_dict[Config.data_frame.iloc[self.val[self.control1]]['targets']]\n",
        "\n",
        "      preprocessing_original_image(img,c,batch,l_cla)\n",
        "\n",
        "      if len(batch) == Config.batch_size:\n",
        "        batch = np.array(batch, np.float32) / 255.\n",
        "        yield {\"inputs\" : np.array(batch, np.float32),\"label\" : np.array(l_cla,np.float32)}\n",
        "        batch = []\n",
        "        l_cla  = []\n",
        "      self.control1 += 1\n",
        "      if self.control1 == self.val.shape[0]:\n",
        "        self.control1 = 0\n",
        "\n",
        "  def test_generator_prepro(self,Config,indice):\n",
        "    batch = []\n",
        "    l_cla = []\n",
        "    img = cv2.imread(Config.data_frame.iloc[indice]['inputs'])\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "    c = Config.class_dict[Config.data_frame.iloc[indice]['targets']]\n",
        "\n",
        "    Config.preprocessing_function(img,c,batch,l_cla)\n",
        "\n",
        "    batch = np.array(batch, np.float32) / 255.\n",
        "    return np.array(batch, np.float32)\n",
        "    batch = []\n",
        "    l_cla  = []\n",
        "  \n",
        "  def test_generator_cru(self,Config,indice):\n",
        "    batch = []\n",
        "    l_cla = []\n",
        "    img = cv2.imread(Config.data_frame.iloc[indice]['inputs'])\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "    c = Config.class_dict[Config.data_frame.iloc[indice]['targets']]\n",
        "\n",
        "    preprocessing_original_image(img,c,batch,l_cla)\n",
        "\n",
        "    batch = np.array(batch, np.float32) / 255.\n",
        "    return np.array(batch, np.float32)\n",
        "    batch = []\n",
        "    l_cla  = []  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TXxEUgLu3nx"
      },
      "source": [
        "##**Model**\n",
        "* Model creation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-m1-i-86U6o"
      },
      "source": [
        "def compiled_model(in_shape, num_classe, base_model, Ploss, opt, Pmetrics):\n",
        "  if (base_model == 0) :\n",
        "    b_model = MobileNetV2(include_top=False, weights='imagenet',input_tensor= Input(shape = in_shape, name = 'inputs'))\n",
        "    # b_model = ResNet50(include_top=False, weights='imagenet',input_tensor= Input(shape = in_shape, name = 'inputs'))\n",
        "  else :\n",
        "    b_model = DenseNet121(include_top=False, weights='imagenet',input_tensor= Input(shape = in_shape, name = 'inputs'))\n",
        "  \n",
        "  if not (Config.freeze):\n",
        "    print('Treinando pesos ResNet')\n",
        "    for layer in b_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "\n",
        "  Rede = b_model.output\n",
        "  Rede = AveragePooling2D()(Rede)\n",
        "  Rede = Flatten()(Rede)\n",
        "  Rede = Dropout(0.4)(Rede) #Dropout(0.5,seed = Config.seed)(Rede)\n",
        "  Rede = Dense(Config.class_quant, activation='softmax',kernel_initializer= Config.initializer, bias_initializer= Config.initializer)(Rede)\n",
        "\n",
        "\n",
        "  Rede = Model(inputs = b_model.input, outputs = Rede)\n",
        "\n",
        "  if (Config.adversarial):\n",
        "    adversarial_config = nsl.configs.make_adv_reg_config(multiplier=0.2, adv_step_size=0.05)\n",
        "    Adv_Rede = nsl.keras.AdversarialRegularization(Rede, adv_config = adversarial_config)#, label_keys=('targets',))\n",
        "    Adv_Rede.compile(loss = Ploss, optimizer = opt, metrics = Pmetrics,run_eagerly = None)\n",
        "    return Adv_Rede\n",
        "    \n",
        "  else:\n",
        "    Rede.compile(loss = Ploss, optimizer = opt, metrics = Pmetrics)#, run_eagerly = None\n",
        "    return Rede"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsGQe4AXhBbK"
      },
      "source": [
        "%load_ext tensorboard\n",
        "# Clear any logs from previous runs\n",
        "# !rm -rf ./logs/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV6ugohzcwCd"
      },
      "source": [
        "Config = Config()\n",
        "\n",
        "np.random.seed(Config.seed)\n",
        "set_seed(Config.seed)\n",
        "random.seed(Config.seed)\n",
        "tf.compat.v1.set_random_seed(Config.seed)\n",
        "tf.random.set_global_generator(tf.random.Generator.from_seed(5))\n",
        "\n",
        "\n",
        "Config.create_data_frame(Config.base_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEhZbR9VihSk"
      },
      "source": [
        "Folds = StratifiedKFold(n_splits = Config.fold_num, shuffle = True, random_state = Config.seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aLp3fSXcdJr"
      },
      "source": [
        "##**Training**\n",
        "* Traning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5cs14iIkL12"
      },
      "source": [
        "zeros = np.zeros(Config.data_frame.shape[0])\n",
        "print('config.dataframe shape: ')\n",
        "print(Config.data_frame.shape[0])\n",
        "n = 0\n",
        "\n",
        "for indice_treinamento, indice_teste in Folds.split(zeros,zeros):\n",
        "\n",
        "  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) # tirar os avisos do tensorflow\n",
        "\n",
        "  Modelo = compiled_model(in_shape = Config.input_shape, num_classe = Config.class_quant, base_model = Config.base_model, Ploss = Config.loss ,opt = Config.optmizer ,Pmetrics = Config.metrics)\n",
        "\n",
        "  gen = Generator(Config, indice_treinamento, indice_teste, Config.validation_split, Config.batch_size)\n",
        "\n",
        "  log_dir = Config.save_path + \"/logs/fit\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq = 1)\n",
        "\n",
        "  if Config.adversarial:\n",
        "    Modelo.fit(\n",
        "      x = gen.adversarial_train_gernerator(Config), \n",
        "      batch_size = Config.batch_size, \n",
        "      epochs = Config.epochs,\n",
        "      steps_per_epoch = int(((Config.img_quant * (1 - Config.validation_split)) / Config.batch_size) + 1),\n",
        "      validation_data = gen.adversarial_val_generator(Config),\n",
        "      validation_steps = int(((Config.img_quant * Config.validation_split) / Config.batch_size) + 1),\n",
        "      callbacks = [tensorboard_callback]\n",
        "      )\n",
        "  else: \n",
        "    Modelo.fit(\n",
        "        x = gen.train_generator(Config), \n",
        "        batch_size = Config.batch_size, \n",
        "        epochs = Config.epochs,\n",
        "        steps_per_epoch = int(((Config.img_quant * (1 - Config.validation_split)) / Config.batch_size) + 1),\n",
        "        validation_data = gen.val_generator(Config),\n",
        "        validation_steps = int(((Config.img_quant * Config.validation_split) / Config.batch_size) + 1),\n",
        "        callbacks = [tensorboard_callback]\n",
        "        )\n",
        "\n",
        "  for id in indice_teste:\n",
        "    Config.predict_function(Modelo.predict(gen.test_generator_cru(Config,id)),id,Config)\n",
        "   \n",
        "  print('OUTRO FOLD : ' + str(n) + \" <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\" )\n",
        "    \n",
        "    \n",
        "  save_model(Modelo, Config.save_path + '/pesos/' + '/pesos-{}.tf'.format(n), save_format = \"tf\")\n",
        "  n +=  1\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "  del Modelo\n",
        "  del gen\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXqlNx5AN_1c"
      },
      "source": [
        "save_mc()\n",
        "save_csv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3_-OBTVmr0_"
      },
      "source": [
        "# %tensorboard --logdir '/content/drive/My Drive/...'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}